{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 – Import necessary packages:\n",
    "To build this Hand Gesture Recognition project, we’ll need four packages. So first import these.\n",
    "# import necessary packages for hand gesture recognition project using Python OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 – Initialize models:\n",
    "Initialize MediaPipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mp.solutions.hands module is responsible for performing hand recognition tasks. To utilize this functionality, we create an instance of the Hands class from this module and store it in the variable mpHands.\n",
    "\n",
    "By using the mpHands.Hands method, we configure the hand recognition model. The parameter max_num_hands specifies the maximum number of hands the model should detect in a single frame. Although MediaPipe is capable of detecting multiple hands at once, our project is set up to detect only one hand at a time.\n",
    "\n",
    "Additionally, the mp.solutions.drawing_utils module provides tools to automatically draw the detected key points on the image, so we don’t need to manually draw them ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained gesture recognition model\n",
    "model_path = 'mp_hand_gesture'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Load gesture class names from a file\n",
    "def load_class_names(file_path):\n",
    "    \"\"\"Read and return a list of class names from a file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "class_names = load_class_names('gesture.names')\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the load_model function to load a pre-trained TensorFlow model. The gesture.names file contains the names of the gesture classes used by the model. To access these class names, we first open the file using Python's built-in open function. We then read the contents of the file with the read() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Step 3 – Read frames from a webcam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained gesture recognition model\n",
    "model_path = 'mp_hand_gesture'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Load gesture class names from a file\n",
    "def load_class_names(file_path):\n",
    "    \"\"\"Read and return a list of class names from a file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "class_names = load_class_names('gesture.names')\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a VideoCapture object and provide the argument 0, which represents the camera ID of the system. In this scenario, 0 corresponds to the default webcam. If you have multiple webcams connected, you can adjust this argument to the appropriate camera ID. Otherwise, you can leave it as is.\n",
    "\n",
    "The cap.read() function captures each frame from the webcam. To modify the frame, we use the cv2.flip() function to flip it horizontally. The cv2.imshow() function displays the frame in a new OpenCV window.\n",
    "\n",
    "The cv2.waitKey() function keeps the window open and actively listens for user input. The window will remain open until the key 'q' is pressed, at which point it will close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Read each frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    x, y, c = frame.shape\n",
    "    \n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Convert frame to RGB\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get hand landmark prediction\n",
    "    result = hands.process(framergb)\n",
    "    \n",
    "    class_name = ''\n",
    "    \n",
    "    # Post-process the result\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for handslms in result.multi_hand_landmarks:\n",
    "            for lm in handslms.landmark:\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "            \n",
    "            # Draw landmarks on frame\n",
    "            mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Predict gesture\n",
    "            prediction = model.predict([landmarks])\n",
    "            class_id = np.argmax(prediction)\n",
    "            class_name = class_names[class_id]\n",
    "    \n",
    "    # Show the prediction on the frame\n",
    "    cv2.putText(frame, class_name, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Output\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "## Step 5 – Recognize hand gestures:\n",
    "\n",
    "  # Predict gesture in Hand Gesture Recognition project\n",
    "            prediction = model.predict([landmarks])\n",
    "print(prediction)\n",
    "            classID = np.argmax(prediction)\n",
    "            className = classNames[classID]\n",
    "  # show the prediction on the frame\n",
    "  cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   1, (0,0,255), 2, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The model.predict() function accepts a list of landmarks as input and produces an array of predictions. Each element in the array represents the likelihood of each of the 10 gesture classes for the given landmarks. The output array looks something like this:\n",
    "  [[2.0691623e-18 1.9585415e-27 9.9990010e-01 9.7559416e-05\n",
    "1.6617223e-06 1.0814080e-18 1.1070732e-27 4.4744065e-16 6.6466129e-07 4.9615162e-21]]\n",
    "The np.argmax() function is then used to determine the index of the highest value in this array, which corresponds to the predicted gesture class. Using this index, we can retrieve the class name from the classNames list.\n",
    "\n",
    "Finally, the cv2.putText() function is used to overlay the detected gesture class name onto the frame, allowing us to visualize the prediction in the displayed video feed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
